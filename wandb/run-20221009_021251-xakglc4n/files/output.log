train epoch:1:   0%|                                                                                                                                                                  | 0/20227 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/torch/nn/modules/loss.py:97: UserWarning: Using a target size (torch.Size([16, 1, 3000])) that is different to the input size (torch.Size([16, 3000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.l1_loss(input, target, reduction=self.reduction)












train epoch:1:   0%|▍                                                                                                                                                      | 56/20227 [00:26<2:38:58,  2.11it/s]
Traceback (most recent call last):
  File "traineval.py", line 63, in <module>
    train_val(trainloader, valloader, testloader)
  File "traineval.py", line 24, in train_val
    checkpoints = trainer.epoch(epoch + 1)
  File "/home/datassd/yilin/Codes/Hand/ConditionHOI/epochbase.py", line 247, in epoch
    self.one_batch(sample)
  File "/home/datassd/yilin/Codes/Hand/ConditionHOI/epochbase.py", line 201, in one_batch
    self.model_update(dict_losses)
  File "/home/datassd/yilin/Codes/Hand/ConditionHOI/epochbase.py", line 175, in model_update
    self.optimizer_cond.step()
  File "/home/yilin/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/home/yilin/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/home/yilin/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/torch/optim/adam.py", line 107, in step
    F.adam(params_with_grad,
  File "/home/yilin/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/torch/optim/_functional.py", line 87, in adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt